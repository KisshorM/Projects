# -*- coding: utf-8 -*-
"""Hate_Speech (1) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fM9hqKm5QvB_v5Cz1mIKdV1OAwjgfnOO
"""

# prompt: mount drive

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df=pd.read_csv('/content/drive/MyDrive/Hate speech/labeled_data.csv')
df.head()

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x=df["class"], palette="Set2")
plt.title("Class Distribution")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

df["text_length"] = df["cleaned_text"].apply(lambda x: len(x.split()))
sns.histplot(df, x="text_length", hue="class", bins=30, kde=True, palette="Set2")
plt.title("Text Length Distribution by Class")
plt.show()

from collections import Counter
import nltk
from wordcloud import WordCloud

def plot_wordcloud(class_label, color):
    words = " ".join(df[df["class"] == class_label]["cleaned_text"])
    wordcloud = WordCloud(width=800, height=400, background_color="white", colormap=color).generate(words)

    plt.figure(figsize=(10,5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"Most Common Words in Class {class_label}")
    plt.show()

for c, color in zip([0,1,2], ["Blues", "Oranges", "Reds"]):
    plot_wordcloud(c, color)

from sklearn.feature_extraction.text import CountVectorizer

def get_top_bigrams(class_label):
    text = df[df["class"] == class_label]["cleaned_text"]
    vectorizer = CountVectorizer(ngram_range=(2,2), stop_words="english").fit(text)
    bigrams = vectorizer.transform(text).sum(axis=0)

    bigram_freq = sorted(zip(vectorizer.get_feature_names_out(), bigrams.tolist()[0]), key=lambda x: x[1], reverse=True)[:10]
    return bigram_freq

for c in [0,1,2]:
    print(f"\nTop 10 Bigrams for Class {c}:")
    print(get_top_bigrams(c))

from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))

df["stopwords_removed"] = df["cleaned_text"].apply(lambda x: " ".join([word for word in x.split() if word not in stop_words]))

print("Before Removing Stopwords:", df["cleaned_text"][0])
print("After Removing Stopwords:", df["stopwords_removed"][0])

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download the necessary NLTK data packages
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab') # Download the punkt_tab resource

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
    text = re.sub(r"@\w+|#\w+", "", text)
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    text = text.lower()
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

df["cleaned_text"] = df["tweet"].apply(clean_text)

from nltk import word_tokenize
from nltk.corpus import wordnet
import random

def synonym_replacement(text):
    words = word_tokenize(text)
    new_words = words.copy()
    for i, word in enumerate(words):
        synonyms = wordnet.synsets(word)
        if synonyms:
            synonym = random.choice(synonyms).lemmas()[0].name()
            new_words[i] = synonym
    return " ".join(new_words)

# Example of augmenting the dataset
df["augmented_text"] = df["cleaned_text"].apply(synonym_replacement)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Include unigrams and bigrams
X = vectorizer.fit_transform(df["cleaned_text"])

!pip install gensim

from gensim.models import Word2Vec

# Tokenize sentences
df["tokens"] = df["cleaned_text"].apply(lambda x: x.split())

# Train Word2Vec model
w2v_model = Word2Vec(sentences=df["tokens"], vector_size=100, window=5, min_count=2, workers=4)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Split the data
from sklearn.model_selection import train_test_split
y = df["class"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# Apply LDA (reduce to 2 components)
lda = LDA(n_components=2)
X_lda = lda.fit_transform(X.toarray(), y)  # LDA needs both features and labels

# Split the transformed dataset
X_train_lda, X_test_lda, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)

# Check dimensions
print("Original feature count:", X.shape[1])
print("Reduced feature count after LDA:", X_lda.shape[1])

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train_lda, y_train)

print("Best parameters:", grid_search.best_params_)

from sklearn.model_selection import cross_val_score

scores = cross_val_score(rf_model, X, y, cv=5)
print("Cross-validation scores:", scores)
print("Mean cross-validation score:", scores.mean())

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression # Import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier # Import GradientBoostingClassifier
# Create individual models
model1 = RandomForestClassifier(n_estimators=100)
model2 = LogisticRegression()
#model3 = GradientBoostingClassifier()

# Create an ensemble model
ensemble_model = VotingClassifier(estimators=[('rf', model1), ('lr', model2)], voting='hard')
ensemble_model.fit(X_train, y_train)

# Predictions
y_pred_ensemble = ensemble_model.predict(X_test)

# Evaluation
print("Ensemble Accuracy:", accuracy_score(y_test, y_pred_ensemble))
print(classification_report(y_test, y_pred_ensemble))

best_params=grid_search.best_params_
rf_model = RandomForestClassifier(**best_params, random_state=42)
rf_model.fit(X_train_lda, y_train)
y_pred_optimized = rf_model.predict(X_test_lda)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred_optimized))
print(classification_report(y_test, y_pred_optimized))

from sklearn.metrics import confusion_matrix
# Confusion Matrix Comparison
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", ax=axes[0])
axes[0].set_title("Before Tuning")
axes[0].set_xlabel("Predicted")
axes[0].set_ylabel("Actual")

sns.heatmap(confusion_matrix(y_test, y_pred_optimized), annot=True, fmt="d", cmap="Greens", ax=axes[1])
axes[1].set_title("After Tuning")
axes[1].set_xlabel("Predicted")
axes[1].set_ylabel("Actual")

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Logistic Regression with L2 regularization
log_reg = LogisticRegression(penalty='l2', C=1.0)  # C is the inverse of regularization strength
log_reg.fit(X_train, y_train)

# Predictions
y_pred_log_reg = log_reg.predict(X_test)

# Evaluation
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_log_reg))
print(classification_report(y_test, y_pred_log_reg))

# Confusion Matrix Plot (Fixed)
plt.figure(figsize=(6, 4))  # Create a new figure
sns.heatmap(confusion_matrix(y_test, y_pred_log_reg), annot=True, fmt="d", cmap="Greens")

plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

# Analyze misclassifications
import pandas as pd

# Get the indices of the test data in the original DataFrame
test_indices = y_test.index

# Create a boolean mask for misclassified examples
misclassified_mask = y_test != y_pred

# Get the misclassified examples from the original DataFrame
misclassified = df.iloc[test_indices[misclassified_mask]]

print(misclassified[['tweet', 'class', 'cleaned_text']])

print(df["class"].value_counts())

new_text = ["nigger", "nice"]  # Example texts
new_text_cleaned = [clean_text(text) for text in new_text]
new_text_vectorized = vectorizer.transform(new_text_cleaned)
prediction = rf_model.predict(new_text_vectorized)
print(prediction)  # Output: predicted class labels

from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB

# Create individual models
rf_model_optimized = RandomForestClassifier(**best_params, random_state=42)  # Optimized RF
log_reg = LogisticRegression(penalty='l2', C=1.0, max_iter=500)
gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)
svc = SVC(kernel='linear', probability=True)

# Create a Voting Classifier (Hard Voting)
ensemble_hard = VotingClassifier(estimators=[
    ('rf', rf_model_optimized), ('lr', log_reg), ('gbm', gbm)
], voting='hard')

ensemble_hard.fit(X_train_lda, y_train)
y_pred_ensemble_hard = ensemble_hard.predict(X_test_lda)

# Create a Voting Classifier (Soft Voting for better probability-based decisions)
ensemble_soft = VotingClassifier(estimators=[
    ('rf', rf_model_optimized), ('lr', log_reg), ('gbm', gbm), ('svc', svc)
], voting='soft')

ensemble_soft.fit(X_train_lda, y_train)
y_pred_ensemble_soft = ensemble_soft.predict(X_test_lda)

# Evaluate hard voting ensemble
print("\nHard Voting Ensemble Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_ensemble_hard))
print(classification_report(y_test, y_pred_ensemble_hard))

# Evaluate soft voting ensemble
print("\nSoft Voting Ensemble Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_ensemble_soft))
print(classification_report(y_test, y_pred_ensemble_soft))

# Confusion Matrix Comparison
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.heatmap(confusion_matrix(y_test, y_pred_optimized), annot=True, fmt="d", cmap="Blues", ax=axes[0])
axes[0].set_title("Optimized Random Forest")
axes[0].set_xlabel("Predicted")
axes[0].set_ylabel("Actual")

sns.heatmap(confusion_matrix(y_test, y_pred_ensemble_hard), annot=True, fmt="d", cmap="Greens", ax=axes[1])
axes[1].set_title("Hard Voting Ensemble")
axes[1].set_xlabel("Predicted")
axes[1].set_ylabel("Actual")

sns.heatmap(confusion_matrix(y_test, y_pred_ensemble_soft), annot=True, fmt="d", cmap="Oranges", ax=axes[2])
axes[2].set_title("Soft Voting Ensemble")
axes[2].set_xlabel("Predicted")
axes[2].set_ylabel("Actual")

plt.show()

# Benchmark comparison with existing literature models
print("\nPerformance Comparison:")
print(f"Optimized Random Forest Accuracy: {accuracy_score(y_test, y_pred_optimized):.4f}")
print(f"Hard Voting Ensemble Accuracy: {accuracy_score(y_test, y_pred_ensemble_hard):.4f}")
print(f"Soft Voting Ensemble Accuracy: {accuracy_score(y_test, y_pred_ensemble_soft):.4f}")

# Compare with baseline models (Random Forest without tuning and Logistic Regression)
rf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)
rf_baseline.fit(X_train_lda, y_train)
y_pred_baseline = rf_baseline.predict(X_test_lda)

log_reg_baseline = LogisticRegression()
log_reg_baseline.fit(X_train_lda, y_train)
y_pred_log_baseline = log_reg_baseline.predict(X_test_lda)

print(f"Baseline Random Forest Accuracy: {accuracy_score(y_test, y_pred_baseline):.4f}")
print(f"Baseline Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_log_baseline):.4f}")

# Conclusion
print("\n📌 Observations:")
print("- Optimized models outperform baseline models.")
print("- Soft voting ensemble performs better than hard voting in most cases.")
print("- Comparing with prior research, ensemble methods (especially boosting and voting) tend to generalize better.")

import joblib

# Save trained model
joblib.dump(rf_model, "hate_speech_model.pkl")

# Save TF-IDF vectorizer
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

# Download the files if using Colab (optional)
from google.colab import files
files.download("hate_speech_model.pkl")
files.download("tfidf_vectorizer.pkl")